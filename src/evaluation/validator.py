
import json
import logging
import numpy as np
from pathlib import Path
from typing import List, Dict, Any
from collections import defaultdict
from sklearn.metrics import cohen_kappa_score
from scipy.stats import kendalltau

from src.evaluation.dice_engine import SimplifiedDICEEvaluator, SimplifiedDICEConfig
from src.utils.ragas_impl import RagasEvaluator, RagasConfig

class DICEValidationEvaluator:
    """DICEéªŒè¯è¯„ä¼°å™¨ - ç”¨äºè¯„ä¼°DICEæœ¬èº«çš„å‡†ç¡®æ€§"""
    
    def __init__(self, config: SimplifiedDICEConfig, tournament_result_file: str = None):
        self.config = config
        self.logger = logging.getLogger("DICEValidation")
        self.dice_evaluator = SimplifiedDICEEvaluator(config)
        self.tournament_result_file = tournament_result_file
        self.tournament_results = None
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logger()
        
        # å¦‚æœæä¾›äº†tournamentç»“æœæ–‡ä»¶ï¼Œåˆ™åŠ è½½å®ƒ
        if self.tournament_result_file and Path(self.tournament_result_file).exists():
            self._load_tournament_results()
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def _load_tournament_results(self):
        """åŠ è½½tournamentç»“æœæ–‡ä»¶"""
        try:
            self.logger.info(f"å¼€å§‹åŠ è½½tournamentç»“æœæ–‡ä»¶: {self.tournament_result_file}")
            with open(self.tournament_result_file, 'r', encoding='utf-8') as f:
                self.tournament_results = json.load(f)
            self.logger.info(f"æˆåŠŸåŠ è½½tournamentç»“æœæ–‡ä»¶ï¼ŒåŒ…å« {len(self.tournament_results.get('swiss_results', {}).get('match_records', []))} ä¸ªå¯¹å†³è®°å½•")
        except Exception as e:
            self.logger.error(f"åŠ è½½tournamentç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            self.tournament_results = None
    
    def _find_tournament_match(self, system_a: str, system_b: str, question: str) -> Dict[str, Any]:
        """åœ¨tournamentç»“æœä¸­æŸ¥æ‰¾åŒ¹é…çš„å¯¹å†³"""
        if not self.tournament_results:
            return None
        
        # æŸ¥æ‰¾åŒ¹é…çš„ç³»ç»Ÿå¯¹
        match_records = self.tournament_results.get('swiss_results', {}).get('match_records', [])
        
        for match in match_records:
            match_system_a = match.get('system_a', '')
            match_system_b = match.get('system_b', '')
            
            # æ£€æŸ¥ç³»ç»Ÿå¯¹æ˜¯å¦åŒ¹é…ï¼ˆè€ƒè™‘é¡ºåºï¼‰
            if ((match_system_a == system_a and match_system_b == system_b) or 
                (match_system_a == system_b and match_system_b == system_a)):
                
                # åœ¨comparisonç»“æœä¸­æŸ¥æ‰¾åŒ¹é…çš„é—®é¢˜
                comparison = match.get('comparison', {})
                question_results = comparison.get('question_results', [])
                
                for q_result in question_results:
                    if q_result.get('question', '') == question:
                        return q_result
        
        return None
    
    def sample_evaluation_pairs(self, qacg_files: List[str], num_samples: int = 200, 
                               random_seed: int = 42) -> List[Dict[str, Any]]:
        """é‡‡æ ·è¯„ä¼°å¯¹"""
        import random
        random.seed(random_seed)
        
        all_pairs = []
        for file_path in qacg_files:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_pairs.extend(data)
        
        if len(all_pairs) < num_samples:
            self.logger.warning(f"å¯ç”¨æ•°æ®å¯¹æ•°é‡({len(all_pairs)})å°‘äºè¯·æ±‚çš„é‡‡æ ·æ•°é‡({num_samples})")
            return all_pairs
        
        return random.sample(all_pairs, num_samples)
    
    def run_dice_evaluation(self, evaluation_pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿è¡ŒDICEè¯„ä¼°"""
        results = []
        
        for i, pair in enumerate(evaluation_pairs):
            try:
                # ä»QACGæ ¼å¼ä¸­æå–é—®ç­”å¯¹
                qa_a = pair.get('qa_a', {})
                qa_b = pair.get('qa_b', {})
                
                question = qa_a.get('question', '')
                system_a = pair.get('system_a', '')
                system_b = pair.get('system_b', '')
                
                # é¦–å…ˆå°è¯•ä»tournamentç»“æœä¸­æŸ¥æ‰¾åŒ¹é…
                tournament_match = self._find_tournament_match(system_a, system_b, question)
                
                if tournament_match:
                    # ä½¿ç”¨tournamentä¸­çš„å·²æœ‰ç»“æœ
                    self.logger.info(f"ä½¿ç”¨tournamentç»“æœ: {system_a} vs {system_b} - {question[:50]}...")
                    
                    passage_judgment = tournament_match.get('passage_judgment', {})
                    score_a = passage_judgment.get('prob_a', 0.0)
                    score_b = passage_judgment.get('prob_b', 0.0)
                    dice_score = score_a - score_b
                    
                    result = {
                        'index': i,
                        'question': question,
                        'system_a': system_a,
                        'system_b': system_b,
                        'answer_a': qa_a.get('rag_answer', ''),
                        'answer_b': qa_b.get('rag_answer', ''),
                        'context_a': qa_a.get('context', []),
                        'context_b': qa_b.get('context', []),
                        'dice_score': dice_score,
                        'dice_explanation': passage_judgment.get('reason', ''),
                        'human_annotation': pair.get('human_annotation', ''),
                        'prob_a': score_a,
                        'prob_b': score_b,
                        'win_type': passage_judgment.get('win_type', 'Unknown'),
                        'source': 'tournament'  # æ ‡è®°æ¥æº
                    }
                else:
                    # æ²¡æœ‰æ‰¾åˆ°tournamentç»“æœï¼Œè¿›è¡Œæ–°çš„æ¨ç†
                    self.logger.info(f"æœªæ‰¾åˆ°tournamentç»“æœï¼Œè¿›è¡Œæ–°æ¨ç†: {system_a} vs {system_b} - {question[:50]}...")
                    
                    # æ„å»ºé—®ç­”å¯¹æ ¼å¼
                    target_qa_a = {
                        'answer': qa_a.get('rag_answer', ''),
                        'context': qa_a.get('context', [])
                    }
                    
                    target_qa_b = {
                        'answer': qa_b.get('rag_answer', ''),
                        'context': qa_b.get('context', [])
                    }
                    
                    # ä½¿ç”¨DICEçš„pairwise judgeè¿›è¡Œè¯„ä¼°
                    judgment = self.dice_evaluator.pairwise_judge.judge_pair(
                        question=question,
                        qa_a=target_qa_a,
                        qa_b=target_qa_b,
                        granularity="passage"  # ä½¿ç”¨passageç²’åº¦è¿›è¡Œè¯„ä¼°
                    )
                    
                    # ä»åˆ¤å†³ç»“æœä¸­æå–åˆ†æ•°
                    passage_judgment = judgment.get('passage_judgment', {})
                    score_a = passage_judgment.get('prob_a', 0.0)
                    score_b = passage_judgment.get('prob_b', 0.0)
                    
                    # è®¡ç®—ç›¸å¯¹åˆ†æ•°ï¼ˆç³»ç»ŸAç›¸å¯¹äºç³»ç»ŸBçš„ä¼˜åŠ¿ï¼‰
                    dice_score = score_a - score_b
                    
                    result = {
                        'index': i,
                        'question': question,
                        'system_a': system_a,
                        'system_b': system_b,
                        'answer_a': qa_a.get('rag_answer', ''),
                        'answer_b': qa_b.get('rag_answer', ''),
                        'context_a': qa_a.get('context', []),
                        'context_b': qa_b.get('context', []),
                        'dice_score': dice_score,
                        'dice_explanation': passage_judgment.get('reason', ''),
                        'human_annotation': pair.get('human_annotation', ''),
                        'prob_a': score_a,
                        'prob_b': score_b,
                        'win_type': passage_judgment.get('win_type', 'Unknown'),
                        'source': 'new_inference'  # æ ‡è®°æ¥æº
                    }
                
                results.append(result)
                
                if (i + 1) % 10 == 0:
                    self.logger.info(f"å·²å®Œæˆ {i + 1}/{len(evaluation_pairs)} ä¸ªè¯„ä¼°")
                    
            except Exception as e:
                self.logger.error(f"è¯„ä¼°ç¬¬{i}ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
                # æ·»åŠ ä¸€ä¸ªé»˜è®¤ç»“æœ
                result = {
                    'index': i,
                    'question': pair.get('qa_a', {}).get('question', ''),
                    'system_a': pair.get('system_a', ''),
                    'system_b': pair.get('system_b', ''),
                    'answer_a': pair.get('qa_a', {}).get('rag_answer', ''),
                    'answer_b': pair.get('qa_b', {}).get('rag_answer', ''),
                    'context_a': pair.get('qa_a', {}).get('context', []),
                    'context_b': pair.get('qa_b', {}).get('context', []),
                    'dice_score': 0.0,
                    'dice_explanation': f'è¯„ä¼°å‡ºé”™: {str(e)}',
                    'human_annotation': pair.get('human_annotation', ''),
                    'prob_a': 0.0,
                    'prob_b': 0.0,
                    'win_type': 'Error',
                    'source': 'error'
                }
                results.append(result)
                continue
        
        return results
    
    def load_human_annotations(self, annotation_file: str) -> Dict[int, str]:
        """åŠ è½½äººå·¥æ ‡æ³¨"""
        annotations = {}
        try:
            with open(annotation_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for item in data:
                    if 'index' in item and 'human_annotation' in item:
                        annotations[item['index']] = item['human_annotation']
        except Exception as e:
            self.logger.error(f"åŠ è½½äººå·¥æ ‡æ³¨æ–‡ä»¶å¤±è´¥: {e}")
        return annotations
    
    def calculate_agreement(self, results: List[Dict[str, Any]], gold_labels: Dict[int, str]) -> Dict[str, float]:
        """è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡"""
        dice_scores = []
        human_scores = []
        
        for result in results:
            if result['index'] in gold_labels:
                dice_scores.append(result['dice_score'])
                # å°†äººå·¥æ ‡æ³¨è½¬æ¢ä¸ºæ•°å€¼åˆ†æ•°
                human_annotation = gold_labels[result['index']]
                if human_annotation.lower() in ['a', 'system_a', 'good', 'correct', 'accurate']:
                    human_scores.append(1.0)  # ç³»ç»ŸAæ›´å¥½
                elif human_annotation.lower() in ['b', 'system_b', 'bad', 'incorrect', 'inaccurate']:
                    human_scores.append(-1.0)  # ç³»ç»ŸBæ›´å¥½
                else:
                    human_scores.append(0.0)  # å¹³å±€æˆ–ä¸­æ€§
        
        if len(dice_scores) == 0:
            return {'correlation': 0.0, 'kappa': 0.0}
        
        # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
        correlation = np.corrcoef(dice_scores, human_scores)[0, 1] if len(dice_scores) > 1 else 0.0
        
        # è®¡ç®—Cohen's Kappa (å°†åˆ†æ•°è½¬æ¢ä¸ºäºŒåˆ†ç±»)
        dice_binary = [1 if score > 0 else 0 for score in dice_scores]  # æ­£æ•°è¡¨ç¤ºAæ›´å¥½
        human_binary = [1 if score > 0 else 0 for score in human_scores]  # æ­£æ•°è¡¨ç¤ºAæ›´å¥½
        kappa = cohen_kappa_score(dice_binary, human_binary) if len(dice_scores) > 1 else 0.0
        
        return {
            'correlation': correlation,
            'kappa': kappa,
            'sample_size': len(dice_scores)
        }
    
    def calculate_elo_correlation(self, results: List[Dict[str, Any]], gold_labels: Dict[int, str]) -> Dict[str, float]:
        """è®¡ç®—ELOç›¸å…³æ€§"""
        # è¿™é‡Œå¯ä»¥å®ç°ELOè¯„åˆ†ç³»ç»Ÿçš„ç›¸å…³æ€§è®¡ç®—
        # æš‚æ—¶è¿”å›åŸºæœ¬çš„ç›¸å…³æ€§æŒ‡æ ‡
        return self.calculate_agreement(results, gold_labels)
    
    def analyze_disagreement_cases(self, results: List[Dict[str, Any]], gold_labels: Dict[int, str]) -> List[Dict[str, Any]]:
        """åˆ†æä¸ä¸€è‡´æ¡ˆä¾‹"""
        disagreement_cases = []
        
        for result in results:
            if result['index'] in gold_labels:
                dice_score = result['dice_score']
                human_annotation = gold_labels[result['index']]
                
                # åˆ¤æ–­æ˜¯å¦ä¸ä¸€è‡´
                dice_a_better = dice_score > 0  # DICEè®¤ä¸ºç³»ç»ŸAæ›´å¥½
                human_a_better = human_annotation.lower() in ['a', 'system_a', 'good', 'correct', 'accurate']
                
                if dice_a_better != human_a_better:
                    disagreement_cases.append({
                        'index': result['index'],
                        'question': result['question'],
                        'system_a': result.get('system_a', ''),
                        'system_b': result.get('system_b', ''),
                        'answer_a': result.get('answer_a', ''),
                        'answer_b': result.get('answer_b', ''),
                        'dice_score': dice_score,
                        'human_annotation': human_annotation,
                        'disagreement_type': 'dice_a_better_human_b_better' if dice_a_better else 'dice_b_better_human_a_better'
                    })
        
        return disagreement_cases
    
    def print_disagreement_analysis(self, disagreement_cases: List[Dict[str, Any]]) -> None:
        """æ‰“å°ä¸ä¸€è‡´åˆ†æ"""
        if not disagreement_cases:
            self.logger.info("æ²¡æœ‰å‘ç°ä¸ä¸€è‡´æ¡ˆä¾‹")
            return
        
        self.logger.info(f"å‘ç° {len(disagreement_cases)} ä¸ªä¸ä¸€è‡´æ¡ˆä¾‹:")
        
        for case in disagreement_cases[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            self.logger.info(f"æ¡ˆä¾‹ {case['index']}: DICEåˆ†æ•°={case['dice_score']:.3f}, äººå·¥æ ‡æ³¨={case['human_annotation']}")
            self.logger.info(f"é—®é¢˜: {case['question'][:100]}...")
    
    def generate_validation_report(self, results: List[Dict[str, Any]], gold_labels: Dict[int, str]) -> Dict[str, Any]:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        agreement_metrics = self.calculate_agreement(results, gold_labels)
        disagreement_cases = self.analyze_disagreement_cases(results, gold_labels)
        
        report = {
            'total_samples': len(results),
            'annotated_samples': len([r for r in results if r['index'] in gold_labels]),
            'agreement_metrics': agreement_metrics,
            'disagreement_count': len(disagreement_cases),
            'disagreement_rate': len(disagreement_cases) / len(results) if results else 0.0,
            'dice_scores_summary': {
                'mean': np.mean([r['dice_score'] for r in results]) if results else 0.0,
                'std': np.std([r['dice_score'] for r in results]) if results else 0.0,
                'min': min([r['dice_score'] for r in results]) if results else 0.0,
                'max': max([r['dice_score'] for r in results]) if results else 0.0
            }
        }
        
        return report


class RagasValidationEvaluator:
    """RAGASéªŒè¯è¯„ä¼°å™¨"""
    
    def __init__(self, config: RagasConfig):
        self.config = config
        self.logger = logging.getLogger("RagasValidation")
        self.ragas_evaluator = RagasEvaluator(config)
        self._setup_logger()
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def run_ragas_evaluation(self, evaluation_pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨RAGASè¯„ä¼°æ‰€æœ‰é‡‡æ ·çš„å¯¹æ¯”å¯¹"""
        self.logger.info(f"å¼€å§‹RAGASè¯„ä¼° {len(evaluation_pairs)} å¯¹æ ·æœ¬")
        
        print(f"\nğŸš€ RAGASæ‰¹é‡è¯„ä¼°å¼€å§‹")
        print(f"ğŸ“Š æ€»å…±éœ€è¦è¯„ä¼°: {len(evaluation_pairs)} å¯¹æ ·æœ¬")
        print("ğŸ”” æ¯æ¬¡è¯„ä¼°ä¼šæ˜¾ç¤ºè¯¦ç»†çš„åˆ¤æ–­è¿‡ç¨‹å’Œç»“æœ")
        print("=" * 120)
        
        ragas_results = []
        for i, pair in enumerate(evaluation_pairs):
            print(f"\nâ³ è¿›åº¦: {i+1}/{len(evaluation_pairs)} ({(i+1)/len(evaluation_pairs)*100:.1f}%)")
            print(f"ğŸ” è¯„ä¼°å¯¹ #{i+1}: {pair['system_a']} vs {pair['system_b']}")
            
            qa_a = pair["qa_a"]
            qa_b = pair["qa_b"]
            
            result = self.ragas_evaluator._pairwise_comparison(
                [qa_a], [qa_b], 
                pair["system_a"], pair["system_b"],
                max_questions=1
            )
            
            if result["question_results"]:
                question_result = result["question_results"][0]
                passage_judgment = question_result.get("passage_judgment", {})
                ragas_details = question_result.get("ragas_details", {})
                
                # æ˜¾ç¤ºæœ¬æ¬¡è¯„ä¼°çš„æœ€ç»ˆç»“æœ
                judgment = passage_judgment.get("label", "Tie")
                score = passage_judgment.get("score", 0.5)
                reason = passage_judgment.get("reason", "")
                
                judgment_icon = "ğŸ†" if judgment != "Tie" else "âš–ï¸"
                print(f"\nâœ… è¯„ä¼°å¯¹ #{i+1} å®Œæˆ:")
                print(f"   {judgment_icon} ç»“æœ: {judgment}")
                print(f"   ğŸ“Š ç½®ä¿¡åº¦: {score:.4f}")
                print(f"   ğŸ“ ç†ç”±: {reason}")
                
                ragas_result = {
                    "pair_id": i,
                    "question": pair["question"],
                    "system_a": pair["system_a"],
                    "system_b": pair["system_b"],
                    "dice_judgment": judgment,
                    "dice_score": score,
                    "dice_reason": reason,
                    "dice_margin_score": passage_judgment.get("margin_score", 0.0),
                    "combined_delta": ragas_details.get("composite_a", 0) - ragas_details.get("composite_b", 0),
                    "ragas_scores_a": ragas_details.get("scores_a", {}),
                    "ragas_scores_b": ragas_details.get("scores_b", {}),
                    "original_pair": pair
                }
            else:
                print(f"\nâŒ è¯„ä¼°å¯¹ #{i+1} å¤±è´¥:")
                print(f"   âš ï¸ RAGASè¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
                
                ragas_result = {
                    "pair_id": i,
                    "question": pair["question"],
                    "system_a": pair["system_a"],
                    "system_b": pair["system_b"],
                    "dice_judgment": "Tie",
                    "dice_score": 0.5,
                    "dice_reason": "RAGASè¯„ä¼°å¤±è´¥",
                    "dice_margin_score": 0.0,
                    "combined_delta": 0.0,
                    "ragas_scores_a": {},
                    "ragas_scores_b": {},
                    "original_pair": pair
                }
            
            ragas_results.append(ragas_result)
            print("â•" * 120)
        
        # æ˜¾ç¤ºæ‰¹é‡è¯„ä¼°ç»Ÿè®¡
        print(f"\nğŸŠ RAGASæ‰¹é‡è¯„ä¼°å®Œæˆï¼")
        print(f"ğŸ“Š è¯„ä¼°ç»Ÿè®¡:")
        
        # ç»Ÿè®¡ç»“æœ
        judgments = [r["dice_judgment"] for r in ragas_results]
        a_wins = judgments.count("A wins")
        b_wins = judgments.count("B wins")
        ties = judgments.count("Tie")
        
        print(f"   ğŸ† A wins: {a_wins} æ¬¡ ({a_wins/len(ragas_results)*100:.1f}%)")
        print(f"   ğŸ† B wins: {b_wins} æ¬¡ ({b_wins/len(ragas_results)*100:.1f}%)")
        print(f"   âš–ï¸ Tie: {ties} æ¬¡ ({ties/len(ragas_results)*100:.1f}%)")
        print("=" * 120)
        
        return ragas_results
    
    def load_human_annotations(self, annotation_file: str) -> Dict[int, str]:
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„æ ‡æ³¨åŠ è½½æ–¹æ³• - è¿™é‡Œéœ€è¦é‡æ–°å®ç°ï¼Œå› ä¸ºä¸èƒ½ä¾èµ–DICEValidationEvaluatorçš„å®ä¾‹"""
        # ç”±äºRagasValidationEvaluatorä¸åŒ…å«load_human_annotationsé€»è¾‘ï¼ˆåœ¨DICEValidationEvaluatorä¸­ï¼‰ï¼Œ
        # æˆ‘ä»¬è¿™é‡Œä¸ºäº†æ¥å£ç»Ÿä¸€ï¼Œç®€å•å®ä¾‹åŒ–ä¸€ä¸ªDICEValidationEvaluatoræ¥è°ƒç”¨
        # æ³¨æ„ï¼šè¿™å¯èƒ½ä¼šå¯¼è‡´ä¸å¿…è¦çš„åˆå§‹åŒ–å¼€é”€ï¼Œä½†åœ¨éªŒè¯è„šæœ¬ä¸­æ˜¯å¯ä»¥æ¥å—çš„
        from src.evaluation.dice_engine import SimplifiedDICEConfig
        temp_dice_evaluator = DICEValidationEvaluator(SimplifiedDICEConfig())
        return temp_dice_evaluator.load_human_annotations(annotation_file)
    
    def calculate_agreement(self, results, gold_labels):
        """ä»£ç†ä¸€è‡´æ€§è®¡ç®—"""
        from src.evaluation.dice_engine import SimplifiedDICEConfig
        temp_dice_evaluator = DICEValidationEvaluator(SimplifiedDICEConfig())
        return temp_dice_evaluator.calculate_agreement(results, gold_labels)
    
    def calculate_elo_correlation(self, results, gold_labels):
        """ä»£ç†Eloç›¸å…³æ€§è®¡ç®—"""
        from src.evaluation.dice_engine import SimplifiedDICEConfig
        temp_dice_evaluator = DICEValidationEvaluator(SimplifiedDICEConfig())
        return temp_dice_evaluator.calculate_elo_correlation(results, gold_labels)
    
    def analyze_disagreement_cases(self, results, gold_labels):
        """ä»£ç†åˆ†æ­§åˆ†æ"""
        from src.evaluation.dice_engine import SimplifiedDICEConfig
        temp_dice_evaluator = DICEValidationEvaluator(SimplifiedDICEConfig())
        return temp_dice_evaluator.analyze_disagreement_cases(results, gold_labels)
    
    def print_disagreement_analysis(self, disagreement_cases):
        """ä»£ç†åˆ†æ­§æ‰“å°"""
        from src.evaluation.dice_engine import SimplifiedDICEConfig
        temp_dice_evaluator = DICEValidationEvaluator(SimplifiedDICEConfig())
        return temp_dice_evaluator.print_disagreement_analysis(disagreement_cases)
    
    def generate_validation_report(self, agreement_metrics, correlation_metrics, results, gold_labels, output_file):
        """ä»£ç†æŠ¥å‘Šç”Ÿæˆ"""
        from src.evaluation.dice_engine import SimplifiedDICEConfig
        temp_dice_evaluator = DICEValidationEvaluator(SimplifiedDICEConfig())
        return temp_dice_evaluator.generate_validation_report(agreement_metrics, correlation_metrics, results, gold_labels, output_file)


class UnifiedValidationEvaluator:
    """ç»Ÿä¸€éªŒè¯è¯„ä¼°å™¨ - æ”¯æŒDICEå’ŒRAGASä¸¤ç§è¯„ä¼°æ–¹æ³•"""
    
    def __init__(self, evaluation_method: str = "dice", dice_config: SimplifiedDICEConfig = None, 
                 ragas_config: RagasConfig = None, tournament_result_file: str = None):
        self.evaluation_method = evaluation_method.lower()
        self.logger = logging.getLogger("UnifiedValidation")
        
        # æ ¹æ®è¯„ä¼°æ–¹æ³•åˆå§‹åŒ–ç›¸åº”çš„è¯„ä¼°å™¨
        if self.evaluation_method == "dice":
            if dice_config is None:
                raise ValueError("ä½¿ç”¨DICEæ–¹æ³•æ—¶å¿…é¡»æä¾›dice_config")
            self.evaluator = DICEValidationEvaluator(dice_config, tournament_result_file)
        elif self.evaluation_method == "ragas":
            if ragas_config is None:
                raise ValueError("ä½¿ç”¨RAGASæ–¹æ³•æ—¶å¿…é¡»æä¾›ragas_config")
            self.evaluator = RagasValidationEvaluator(ragas_config)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¯„ä¼°æ–¹æ³•: {evaluation_method}")
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logger()
        
        self.logger.info(f"åˆå§‹åŒ–ç»Ÿä¸€éªŒè¯è¯„ä¼°å™¨ï¼Œä½¿ç”¨æ–¹æ³•: {self.evaluation_method.upper()}")
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def _derive_dice_label(self, result: Dict[str, Any]) -> str:
        """ç»Ÿä¸€æ¨æ–­DICEæ ‡ç­¾çš„é€»è¾‘ï¼Œé¿å…åˆ†å€¼å°ºåº¦è¯¯åˆ¤å¯¼è‡´ç»Ÿè®¡é”™è¯¯ã€‚"""
        explicit_label = result.get("dice_judgment")
        if explicit_label in {"A wins", "B wins", "Tie"}:
            return explicit_label
        
        score = result.get("dice_score")
        if isinstance(score, (int, float)):
            # è‹¥æ˜¯[0,1]åˆ†å°ºåº¦ï¼Œåˆ™ä»¥0.5ä¸ºä¸­æ€§é˜ˆå€¼ï¼ŒåŠ å…¥è½»å¾®ç¼“å†²
            if 0.0 <= score <= 1.0:
                if score > 0.55:
                    return "A wins"
                if score < 0.45:
                    return "B wins"
                return "Tie"
            # å¦åˆ™è§†ä¸ºå¯¹ç§°åˆ†åˆ¶ï¼ˆå¦‚[-1,1]ï¼‰ï¼Œä»¥0ä¸ºä¸­æ€§é˜ˆå€¼ï¼ŒåŠ å…¥è½»å¾®ç¼“å†²
            if score > 0.1:
                return "A wins"
            if score < -0.1:
                return "B wins"
            return "Tie"
        
        # å›é€€ï¼šè‹¥æœ‰prob_a/prob_bå¯æ¯”è¾ƒ
        prob_a = result.get("prob_a")
        prob_b = result.get("prob_b")
        if isinstance(prob_a, (int, float)) and isinstance(prob_b, (int, float)):
            delta = prob_a - prob_b
            if delta > 0.05:
                return "A wins"
            if delta < -0.05:
                return "B wins"
            return "Tie"
        
        return "Tie"
        
    def sample_evaluation_pairs(self, qacg_files: List[str], num_samples: int = 200, 
                               random_seed: int = 42) -> List[Dict[str, Any]]:
        """
        ä»70é¢˜ä¸­éšæœºæŠ½å–200å¯¹(q, cA, aA, cB, aB)ç”¨äºäººå·¥æ ‡æ³¨
        
        Args:
            qacg_files: QACGæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            num_samples: é‡‡æ ·æ•°é‡
            random_seed: éšæœºç§å­
            
        Returns:
            é‡‡æ ·çš„è¯„ä¼°å¯¹åˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹é‡‡æ · {num_samples} å¯¹è¯„ä¼°æ ·æœ¬")
        import random
        random.seed(random_seed)
        
        # åŠ è½½æ‰€æœ‰ç³»ç»Ÿæ•°æ®
        all_systems_data = {}
        for file_path in qacg_files:
            system_name = Path(file_path).stem.replace("qacg_", "")
            with open(file_path, 'r', encoding='utf-8') as f:
                all_systems_data[system_name] = json.load(f)
        
        systems = list(all_systems_data.keys())
        if len(systems) < 2:
            raise ValueError(f"éœ€è¦è‡³å°‘2ä¸ªç³»ç»Ÿï¼Œå®é™…è·å¾—{len(systems)}ä¸ª")
        
        self.logger.info(f"åŠ è½½äº† {len(systems)} ä¸ªç³»ç»Ÿ: {systems}")
        
        # ç¡®å®šæ•°æ®é•¿åº¦ï¼ˆä½¿ç”¨æœ€çŸ­çš„ç³»ç»Ÿæ•°æ®é•¿åº¦ï¼‰
        min_length = min(len(data) for data in all_systems_data.values())
        self.logger.info(f"æ¯ä¸ªç³»ç»Ÿæœ‰ {min_length} é¢˜æ•°æ®")
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç³»ç»Ÿå¯¹å’Œé¢˜ç›®ç»„åˆ
        all_combinations = []
        for i, system_a in enumerate(systems):
            for j, system_b in enumerate(systems):
                if i < j:  # é¿å…é‡å¤å¯¹æ¯”
                    for q_idx in range(min_length):
                        qa_a = all_systems_data[system_a][q_idx]
                        qa_b = all_systems_data[system_b][q_idx]
                        
                        # ç¡®ä¿ä¸¤ä¸ªç³»ç»Ÿå›ç­”çš„æ˜¯åŒä¸€ä¸ªé—®é¢˜
                        if qa_a["question"] == qa_b["question"]:
                            combination = {
                                "question_idx": q_idx,
                                "system_a": system_a,
                                "system_b": system_b,
                                "qa_a": qa_a,
                                "qa_b": qa_b,
                                "question": qa_a["question"],
                                "answer_a": qa_a.get("rag_answer", ""),
                                "answer_b": qa_b.get("rag_answer", ""),
                                "expected_answer": qa_a.get("expected_answer", ""),
                                "context_a": qa_a.get("context", []),
                                "context_b": qa_b.get("context", []),
                                "groundtruth": qa_a.get("groundtruth", qa_a.get("expected_answer", ""))
                            }
                            all_combinations.append(combination)
        
        self.logger.info(f"æ€»å…±æœ‰ {len(all_combinations)} ä¸ªå¯èƒ½çš„ç»„åˆ")
        
        # éšæœºé‡‡æ ·
        if len(all_combinations) < num_samples:
            self.logger.warning(f"å¯ç”¨ç»„åˆæ•° ({len(all_combinations)}) å°‘äºéœ€æ±‚æ ·æœ¬æ•° ({num_samples})")
            sampled_pairs = all_combinations
        else:
            sampled_pairs = random.sample(all_combinations, num_samples)
        
        self.logger.info(f"æˆåŠŸé‡‡æ · {len(sampled_pairs)} å¯¹è¯„ä¼°æ ·æœ¬")
        return sampled_pairs
    
    def run_evaluation(self, evaluation_pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿è¡Œç›¸åº”çš„è¯„ä¼°æ–¹æ³•"""
        if self.evaluation_method == "dice":
            return self.run_dice_evaluation(evaluation_pairs)
        elif self.evaluation_method == "ragas":
            return self.evaluator.run_ragas_evaluation(evaluation_pairs)
    
    def load_human_annotations(self, annotation_file: str) -> Dict[int, str]:
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„æ ‡æ³¨åŠ è½½æ–¹æ³•"""
        return self.evaluator.load_human_annotations(annotation_file)
    
    def calculate_agreement(self, results: List[Dict[str, Any]], 
                          gold_labels: Dict[int, str]) -> Dict[str, float]:
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„ä¸€è‡´æ€§è®¡ç®—æ–¹æ³•"""
        return self.evaluator.calculate_agreement(results, gold_labels)
    
    def calculate_elo_correlation(self, results: List[Dict[str, Any]], 
                                gold_labels: Dict[int, str]) -> Dict[str, float]:
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„Eloç›¸å…³æ€§è®¡ç®—æ–¹æ³•"""
        return self.evaluator.calculate_elo_correlation(results, gold_labels)
    
    def analyze_disagreement_cases(self, results: List[Dict[str, Any]], 
                                  gold_labels: Dict[int, str]) -> List[Dict[str, Any]]:
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„åˆ†æ­§åˆ†ææ–¹æ³•"""
        return self.evaluator.analyze_disagreement_cases(results, gold_labels)
    
    def print_disagreement_analysis(self, disagreement_cases: List[Dict[str, Any]]):
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„åˆ†æ­§æ‰“å°æ–¹æ³•"""
        return self.evaluator.print_disagreement_analysis(disagreement_cases)
    
    def generate_validation_report(self, agreement_metrics: Dict[str, Any], 
                                 correlation_metrics: Dict[str, Any],
                                 results: List[Dict[str, Any]],
                                 gold_labels: Dict[int, str],
                                 output_file: str):
        """ä»£ç†åˆ°å…·ä½“è¯„ä¼°å™¨çš„æŠ¥å‘Šç”Ÿæˆæ–¹æ³•"""
        return self.evaluator.generate_validation_report(
            agreement_metrics, correlation_metrics, results, gold_labels, output_file
        )
    
    def run_dice_evaluation(self, evaluation_pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨DICEè¯„ä¼°æ‰€æœ‰é‡‡æ ·çš„å¯¹æ¯”å¯¹
        
        Args:
            evaluation_pairs: è¯„ä¼°å¯¹åˆ—è¡¨
            
        Returns:
            DICEè¯„ä¼°ç»“æœåˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹DICEè¯„ä¼° {len(evaluation_pairs)} å¯¹æ ·æœ¬")
        
        dice_results = []
        for i, pair in enumerate(evaluation_pairs):
            self.logger.info(f"è¯„ä¼°ç¬¬ {i+1}/{len(evaluation_pairs)} å¯¹")
            
            # ä½¿ç”¨DICEè¿›è¡Œè¯„ä¼°
            qa_a = pair["qa_a"]
            qa_b = pair["qa_b"]
            
            # ä½¿ç”¨DICEè¯„ä¼°å™¨çš„_pairwise_comparisonæ–¹æ³•
            result = self.evaluator.dice_evaluator._pairwise_comparison(
                [qa_a], [qa_b], 
                pair["system_a"], pair["system_b"],
                max_questions=1
            )
            
            # æå–å…³é”®ä¿¡æ¯
            if result["question_results"]:
                question_result = result["question_results"][0]
                passage_judgment = question_result.get("passage_judgment", {})
                
                dice_result = {
                    "pair_id": i,  # ä½¿ç”¨ç´¢å¼•ä½œä¸ºpair_idï¼Œä¸æ ‡æ³¨æ¨¡æ¿ä¿æŒä¸€è‡´
                    "question": pair["question"],
                    "system_a": pair["system_a"],
                    "system_b": pair["system_b"],
                    "dice_judgment": passage_judgment.get("label", "Tie"),
                    "dice_score": passage_judgment.get("score", 0.5),
                    "dice_reason": passage_judgment.get("reason", ""),
                    "dice_margin_score": passage_judgment.get("margin_score", 0.0),
                    "combined_delta": question_result.get("elo_delta", 0.0),
                    "original_pair": pair
                }
            else:
                # å¤‡ç”¨ç»“æœ
                dice_result = {
                    "pair_id": i,  # ä½¿ç”¨ç´¢å¼•ä½œä¸ºpair_idï¼Œä¸æ ‡æ³¨æ¨¡æ¿ä¿æŒä¸€è‡´
                    "question": pair["question"],
                    "system_a": pair["system_a"],
                    "system_b": pair["system_b"],
                    "dice_judgment": "Tie",
                    "dice_score": 0.5,
                    "dice_reason": "è¯„ä¼°å¤±è´¥",
                    "dice_margin_score": 0.0,
                    "combined_delta": 0.0,
                    "original_pair": pair
                }
            
            dice_results.append(dice_result)
        
        return dice_results
    
    def _create_annotation_template(self, annotation_file: str):
        """åˆ›å»ºäººå·¥æ ‡æ³¨æ¨¡æ¿æ–‡ä»¶"""
        self.logger.info(f"åˆ›å»ºæ ‡æ³¨æ¨¡æ¿: {annotation_file}")
        
        template = {
            "instructions": "è¯·3ä½ä¸“å®¶ç‹¬ç«‹å®Œæˆæ ‡æ³¨ï¼Œæ¯ä¸ªpair_idå¯¹åº”ä¸€ä¸ªè¯„ä¼°å¯¹ï¼Œè¯·ä¸ºæ¯ä½ä¸“å®¶åœ¨expert_votesä¸­å¡«å…¥ 'A wins'ã€'B wins' æˆ– 'Tie'",
            "annotation_guide": {
                "A wins": "ç³»ç»ŸAæ˜æ˜¾ä¼˜äºç³»ç»ŸB",
                "B wins": "ç³»ç»ŸBæ˜æ˜¾ä¼˜äºç³»ç»ŸA", 
                "Tie": "ä¸¤ä¸ªç³»ç»Ÿè¡¨ç°ç›¸å½“ï¼Œéš¾ä»¥åŒºåˆ†ä¼˜åŠ£"
            },
            "annotations": [
                {
                    "pair_id": 0,
                    "question": "ç¤ºä¾‹é—®é¢˜",
                    "system_a": "system_a_name",
                    "answer_a": "ç³»ç»ŸAçš„å›ç­”",
                    "system_b": "system_b_name", 
                    "answer_b": "ç³»ç»ŸBçš„å›ç­”",
                    "expert_votes": ["A wins", "B wins", "A wins"]  # 3ä½ä¸“å®¶çš„æŠ•ç¥¨
                }
            ]
        }
        
        with open(annotation_file, 'w', encoding='utf-8') as f:
            json.dump(template, f, ensure_ascii=False, indent=2)
    
    def _generate_conclusion(self, agreement_metrics: Dict[str, Any], 
                           correlation_metrics: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»“è®º"""
        kappa = agreement_metrics["kappa"]
        tau = correlation_metrics["kendall_tau"]
        
        # æ£€æŸ¥æ˜¯å¦ä¸º2ç³»ç»Ÿçš„ç‰¹æ®Šæƒ…å†µ
        num_systems = len(correlation_metrics.get("dice_ranking", []))
        if num_systems == 2:
            if tau == -1.0:
                conclusion = "ğŸ“Š 2ç³»ç»ŸéªŒè¯ï¼šDICEä¸äººå·¥æ’åºå®Œå…¨ç›¸åï¼ˆÏ„=-1.0ï¼‰ã€‚"
                if kappa >= 0.6:
                    conclusion += f"ä½†Îºå€¼({kappa:.3f})è¡¨æ˜æ€»ä½“ä¸€è‡´æ€§å°šå¯ï¼Œå¯èƒ½å­˜åœ¨ç³»ç»Ÿåå¥½å·®å¼‚ã€‚"
                else:
                    conclusion += f"ä¸”Îºå€¼({kappa:.3f})è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥åˆ¤å†³é€»è¾‘æˆ–å¢åŠ æ›´å¤šç³»ç»Ÿè¿›è¡ŒéªŒè¯ã€‚"
                return conclusion
            elif tau == 1.0:
                return f"âœ… 2ç³»ç»ŸéªŒè¯ï¼šDICEä¸äººå·¥æ’åºå®Œå…¨ä¸€è‡´ï¼ˆÏ„=1.0ï¼‰ï¼ŒÎºå€¼={kappa:.3f}ã€‚"
        
        # æ ‡å‡†çš„å¤šç³»ç»Ÿè¯„ä¼°
        if kappa >= 0.85 and tau >= 0.9:
            return "âœ… DICEç³»ç»ŸéªŒè¯é€šè¿‡ï¼Îºå€¼å’ŒKendall-Ï„å‡è¾¾æ ‡ï¼Œç³»ç»Ÿå¯ä¿¡åº¦é«˜ï¼Œå¯ç”¨äºåç»­è¯„ä¼°ã€‚"
        elif kappa >= 0.85:
            return "âš ï¸ DICEç³»ç»Ÿéƒ¨åˆ†é€šè¿‡ã€‚Îºå€¼è¾¾æ ‡ä½†æ’åºç›¸å…³æ€§ä¸è¶³ï¼Œå»ºè®®æ£€æŸ¥Eloè®¡ç®—é€»è¾‘ã€‚"
        elif tau >= 0.9:
            return "âš ï¸ DICEç³»ç»Ÿéƒ¨åˆ†é€šè¿‡ã€‚æ’åºç›¸å…³æ€§è¾¾æ ‡ä½†ä¸€è‡´æ€§ä¸è¶³ï¼Œå»ºè®®æ£€æŸ¥åˆ¤å†³é€»è¾‘ã€‚"
        else:
            return "âŒ DICEç³»ç»ŸéªŒè¯å¤±è´¥ã€‚Îºå€¼å’ŒKendall-Ï„å‡æœªè¾¾æ ‡ï¼Œéœ€è¦é‡æ–°è°ƒæ•´è¯„ä¼°ç­–ç•¥ã€‚"
    
    def _print_validation_summary(self, report: Dict[str, Any]):
        """æ‰“å°éªŒè¯æ‘˜è¦"""
        summary = report["validation_summary"]
        
        print("\n" + "="*60)
        print("ğŸ”¬ DICEç³»ç»ŸéªŒè¯ç»“æœ")
        print("="*60)
        print(f"Îº å€¼ (ç›®æ ‡â‰¥0.85): {summary['kappa_score']:.3f}")
        print(f"å‡†ç¡®ç‡: {summary['accuracy']:.3f}")
        print(f"Kendall-Ï„ (ç›®æ ‡â‰¥0.9): {summary['kendall_tau']:.3f}")
        print(f"éªŒè¯çŠ¶æ€: {'âœ… é€šè¿‡' if summary['validation_passed'] else 'âŒ æœªé€šè¿‡'}")
        print("\n" + report["conclusion"])
        print("="*60)

